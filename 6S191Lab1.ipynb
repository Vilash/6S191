{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6S191Lab1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGdO8T6Ja7lKhHklrze1aJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uro2fo4fHSqQ"
      },
      "source": [
        "# **1. Install Tensorflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flv3Z3xiG4Y2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Download and import MIT 6.S191 package\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqCJvckBHxFR"
      },
      "source": [
        "# **2. Why is TensorFlow called TensorFlow**\n",
        "\n",
        "* it handles flow of Tensors\n",
        "* Tensors are multidimensional arrays that generalize vectors and matrices \n",
        "* it's shape defines number of dimensions and size of each dimension\n",
        "\n",
        "a 0-d Tensor, scalar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ibzJRXItOD"
      },
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "\n",
        "print(\"`sport` is a {} - dimensional Tensor\".format(tf.rank(sport).numpy()))\n",
        "print(\"`number` is a {} - dimensional Tensor\".format(tf.rank(number).numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjNjY61DJSIr"
      },
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcK0fZgJjye"
      },
      "source": [
        "### Defining higher-order Tensors ###\n",
        "\n",
        "matrix = tf.constant([[3.141592, 1.414213],\n",
        "                      [3.141592, 1.414213],\n",
        "                      [3.141592, 1.414213]], tf.float64)\n",
        "\n",
        "print(\"`matrix` is a {}-d Tensor with shape {}\".format(tf.rank(matrix).numpy(), tf.shape(matrix).numpy()))\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CsIyHEL5k6"
      },
      "source": [
        "- the base `tf.Tensor` class requires tensors to be \"**rectangular**\"\n",
        "- every element of same size\n",
        "- specialized tensors can have different shapes\n",
        " - ragged tensors\n",
        " - sparse tensors\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KHLSgy4K1Dd"
      },
      "source": [
        "a = tf.constant([[1,2],\n",
        "                 [3,4]], tf.int32)\n",
        "b = tf.constant([[1,1],\n",
        "                 [1,1]])\n",
        "\n",
        "print(tf.add(a, b), \"\\nAddition\\n\")\n",
        "print(tf.multiply(a, b), \"\\nMultiplication\\n\")\n",
        "print(tf.matmul(a,b), \"\\nMatrix Multiplication\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqF8Kfh8MzYz"
      },
      "source": [
        "images = tf.zeros([10, 256, 256, 3])\n",
        "\n",
        "assert isinstance(images, tf.Tensor)\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxjTse8RPbQ1"
      },
      "source": [
        "# **3. Computations on Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYmS-GxEP7wd"
      },
      "source": [
        "def func(a,b):\n",
        "  c = a + b\n",
        "  d = b - 1\n",
        "  e = c * d\n",
        "  return e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfgSTzT8QG0H"
      },
      "source": [
        "a, b = 1.5, 2.5\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5FxlrBWQUQr"
      },
      "source": [
        "## **4. Neural networkds in TensorFlow**\n",
        "\n",
        "- Below we consider the example of a simple perceptron defined by just one dense layer: $ y = \\sigma(Wx + b)$, where $W$ represents a matrix of weights, $b$ is a bias, $x$ is the input, $\\sigma$ is the sigmoid activation function, and $y$ is the output. \n",
        "- Tensors can flow through abstract types called Layers -- the building blocks of neural networks. Layers implement common neural networks operations, and are used to update weights, compute losses, and define inter-layer connectivity. We will first define a Layer to implement the simple perceptron defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlCT8teQuc8"
      },
      "source": [
        "# n_output_nodes: number of output nodes\n",
        "# input_shape: shape of input\n",
        "# x: input to the layer\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    d = int(input_shape[-1])\n",
        "\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes])\n",
        "    # print(self.W)\n",
        "    # print(d)\n",
        "    # print(self.n_output_nodes)\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes])\n",
        "    # print(self.b)\n",
        "  def call(self, x):\n",
        "    z = tf.add(tf.matmul(x, self.W), self.b)\n",
        "    y = tf.sigmoid(z)\n",
        "\n",
        "    return y\n",
        "\n",
        "# for reproducing the output set random seed\n",
        "tf.random.set_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1,2))\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "print(y)\n",
        "mdl.lab1.test_custom_dense_layer_output(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WMWzIYqa4SA"
      },
      "source": [
        "- TensorFlow has defined a number of Layers that are commonly used in neural networks, for example a Dense.\n",
        "- \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET3XwHz0TjR-"
      },
      "source": [
        "### Defining a neural network using the Sequential API ###\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "n_output_nodes = 3\n",
        "\n",
        "model = Sequential()\n",
        "dense_layer = Dense(n_output_nodes, activation=\"sigmoid\")\n",
        "\n",
        "# add dense layer to the model\n",
        "model.add(dense_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3I4QhgJUY1j"
      },
      "source": [
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "model_output = model(x_input)\n",
        "print(model_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGzAOCYQdeip"
      },
      "source": [
        "In addition to defining models using the Sequential API, we can also define neural networks by directly subclassing the Model class, which groups layers together to enable model training and inference. The Model class captures what we refer to as a \"model\" or as a \"network\". Using Subclassing, we can create a class for our model, and then define the forward pass through the network using the call function. Subclassing affords the flexibility to define custom layers, custom training loops, custom activation functions, and custom models. Let's define the same neural network as above now using Subclassing rather than the Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50QRl2FcxoJ"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    self.dense_layer = Dense(n_output_nodes, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7te-J7geGzT"
      },
      "source": [
        "n_output_nodes = 3\n",
        "\n",
        "model = SubclassModel(n_output_nodes)\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaAggOtSfZpY"
      },
      "source": [
        "Importantly, Subclassing affords us a lot of flexibility to define custom models. For example, we can use boolean arguments in the call function to specify different network behaviors, for example different behaviors during training and inference. Let's suppose under some instances we want our network to simply output the input, without any perturbation. We define a boolean argument isidentity to control this behavior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4isph_leVj0"
      },
      "source": [
        "### Defining a model using subclassing and specifying custom behavior ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    if isidentity:\n",
        "      return inputs\n",
        "    x = self.dense_layer(inputs)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww0C7k4dhPZG"
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "out_activate = model(x_input, False)\n",
        "out_identity = model(x_input, True)\n",
        "\n",
        "print(\"Network output with activation: {}\\n network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q59eLPbfiXav"
      },
      "source": [
        "## **5. Automatic differentiation in TensorFlow**\n",
        "\n",
        "- all forward pass operations are recorded to a gradient tape\n",
        "- the gradient tape is played backwards to **compute gradient**\n",
        "- after it is played backwards once, the tape is discarded\n",
        "- **persitent gradient tapes** are used to compute multiple gradients over same computation\n",
        "\n",
        "\n",
        "`tf.GradientTape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjRfJaPIhsLN"
      },
      "source": [
        "### Gradient computation with GradientTape ###\n",
        "\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * x\n",
        "\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "print(dy_dx)\n",
        "assert dy_dx.numpy() == 6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5JguMv6lt2N"
      },
      "source": [
        "In training neural networks, we use differentiation and stochastic gradient descent (SGD) to optimize a loss function. Now that we have a sense of how GradientTape can be used to compute and access derivatives, we will look at an example where we use automatic differentiation and SGD to find the minimum of $L=(x-x_f)^2$. Here $x_f$ is a variable for a desired value we are trying to optimize for; $L$ represents a loss that we are trying to minimize. While we can clearly solve this problem analytically ($x_{min}=x_f$), considering how we can compute this using GradientTape sets us up nicely for future labs where we use gradient descent to optimize entire neural network losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVan3b8Dkh-6"
      },
      "source": [
        "### Function minimization with automatic differentiation and SGD ###\n",
        "\n",
        "# Initialize a random value for our initial x\n",
        "# x = tf.Variable([tf.random.normal([1])])\n",
        "x = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-1  # learning_rate for SGD\n",
        "x_f = 4               # target value\n",
        "history = []\n",
        "\n",
        "# run SGD for a number of iterations. \n",
        "# compute derivative of loss at each iteration.\n",
        "# perform SGD update\n",
        "for i in range(500):\n",
        "  if x != x_f:\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = (x_f - x)**2\n",
        "\n",
        "    grad = tape.gradient(loss, x)     # compute derivative of loss wrt x\n",
        "    new_x = x - learning_rate * grad  # sgd update\n",
        "    x.assign(new_x)                   # update x\n",
        "    history.append(x.numpy()[0])\n",
        "    # history.append(x.numpy())\n",
        "\n",
        "print(i)\n",
        "print(history)\n",
        "# plot evolution of x as we optimize towards x_f\n",
        "plt.plot(history)\n",
        "plt.plot([-100,500], [x_f, x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kp9js0otaOE"
      },
      "source": [
        "# **Music Generation with RNNs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuWytQwfpVhB"
      },
      "source": [
        "import os \n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "\n",
        "len(tf.config.list_physical_devices('GPU')) > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGby0WYIuDJ3"
      },
      "source": [
        "# Download Irish folk songs represented in ABC notation, Dataset.\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# check one of the songs\n",
        "example_song = songs[5]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52WLG7hzzzQ1"
      },
      "source": [
        "print(songs[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6vasrzlz_o1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgDL9puum7R"
      },
      "source": [
        "# Convert song from ABC notation to an audio file and play it\n",
        "mdl.lab1.play_song(example_song)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPbN0Gau9Tg"
      },
      "source": [
        "# join all songs in a single string\n",
        "songs_joined = \"\\n\\n\".join(songs)\n",
        "\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO35mxDe3m1a"
      },
      "source": [
        "Let's take a step back and consider our prediction task. We're trying to train a RNN model to learn patterns in ABC music, and then use this model to generate (i.e., predict) a new piece of music based on this learned information.\n",
        "\n",
        "Breaking this down, what we're really asking the model is: given a character, or a sequence of characters, what is the most probable next character? We'll train the model to perform this task.\n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBQi0lgS3s0R"
      },
      "source": [
        "# **Vectorize the text**\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbtjwR43IvS"
      },
      "source": [
        "# mapping from unique character to unique index\n",
        "#   we can evaluate `char2idx[\"d\"]`\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "# Create a mapping from unique index to unique character\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G75ZSgn4fp9"
      },
      "source": [
        "take a peek at this numerical representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHay-0Cc4kYe"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "# same thing as above\n",
        "for char in char2idx:\n",
        "  print(\" {:4s}: {:3d},\".format(repr(char), char2idx[char]))\n",
        "print(\"...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAIhvaYx4rU0"
      },
      "source": [
        "def vectorize_string(string):\n",
        "  # return np.array(char2idx[char] for char in string)\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output\n",
        "  \n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwrfXYjd6i9f"
      },
      "source": [
        "print(vectorized_songs[:10])\n",
        "for num in vectorized_songs[:10]:\n",
        "  print(\"{} {}\".format(num, repr(idx2char[num])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YtowLW84rV"
      },
      "source": [
        "# **Create training examples and targets**\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. Each input sequence that we feed into our RNN will contain seq_length characters from the text. We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of seq_length+1. Suppose seq_length is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\" and the target sequence is \"ello\".\n",
        "\n",
        "The batch method will then let us convert this stream of character indices to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knbE6VP36sns"
      },
      "source": [
        "n = vectorized_songs.shape[0] - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6CSEXR0_1yZ"
      },
      "source": [
        "print(np.random.choice(200678-5, 5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79rRxTb19g18"
      },
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "  # the length of the vectorized songs string\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "  # check how much difference would it make to not have -1 above   \n",
        "  # n = vectorized_songs.shape[0]\n",
        "  # randomly choose the starting indices for the examples in the training batch\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "  # print(idx)\n",
        "  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
        "  # print(input_batch,\"\\n\")\n",
        "  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
        "  # print(output_batch,\"\\n\")\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "  # print([idx2char[i] for i in x_batch],\"\\n\")\n",
        "  # print(y_batch.shape,\"\\n\")\n",
        "  return x_batch, y_batch\n",
        "\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "\n",
        "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args): \n",
        "   print(\"======\\n[FAIL] could not pass tests\")\n",
        "else: \n",
        "   print(\"======\\n[PASS] passed all tests!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ouqepCqJhLV"
      },
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing, but additionally, the RNN considers the information from the previous step, i.e., its updated state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjZbSYs_CO_P"
      },
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ifhVAYERYG"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "  print(\"Step: {:3d}\".format(i))\n",
        "  print(\"input: {} {:s}\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\"output: {} {:s}\\n\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMIg72_fKUI3"
      },
      "source": [
        "def LSTM(rnn_units):\n",
        "  return tf.keras.layers.LSTM(\n",
        "      rnn_units,\n",
        "      return_sequences=True,\n",
        "      recurrent_initializer='glorot_uniform',\n",
        "      recurrent_activation='sigmoid',\n",
        "      stateful=True,\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5fDx9uDRC7t"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      LSTM(rnn_units),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5C70SmRUeTa"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij_DlU2DUlAs"
      },
      "source": [
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
        "\n",
        "pred = model(x)\n",
        "\n",
        "# print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(x[0,0],\"\\n\")\n",
        "print(pred[0,0],\"\\n\")\n",
        "# print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDSPXksDZWIU"
      },
      "source": [
        "## **Predictions from the untrained model**\n",
        "Let's take a look at what our untrained model is predicting.\n",
        "\n",
        "To get actual predictions from the model, we sample from the output distribution, which is defined by a softmax over our character vocabulary. This will give us actual character indices. This means we are using a categorical distribution to sample over the example prediction. This gives a prediction of the next character (specifically its index) at each timestep.\n",
        "\n",
        "Note here that we sample from this probability distribution, as opposed to simply taking the argmax, which can cause the model to get stuck in a loop.\n",
        "\n",
        "Let's try this sampling out for the first example in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMavBZFpVP_O"
      },
      "source": [
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMIiN0kqZgSO"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_p8C4JUbihw"
      },
      "source": [
        "# **Training the model: loss and training operations**\n",
        " Now it's time to train the model!\n",
        "\n",
        "At this point, we can think of our next character prediction problem as a standard classification problem. Given the previous state of the RNN, as well as the input at a given time step, we want to predict the class of the next character -- that is, to actually predict the next character.\n",
        "\n",
        "To train our model on this classification task, we can use a form of the crossentropy loss (negative log likelihood loss). Specifically, we will use the sparse_categorical_crossentropy loss, as it utilizes integer targets for categorical classification tasks. We will want to compute the loss using the true targets -- the labels -- and the predicted targets -- the logits.\n",
        "\n",
        "Let's first compute the loss using our example predictions from the untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSWvDPrGa5G8"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  return loss\n",
        "\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "\n",
        "print(\"Scalar Loss: {}\".format(example_batch_loss.numpy().mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSICJWuacQwh"
      },
      "source": [
        "### Hyperparameter setting and optimization ###\n",
        "\n",
        "# Optimization parameters:\n",
        "num_training_iterations = 2000  # Increase this to train longer\n",
        "batch_size = 4  # Experiment between 1 and 64\n",
        "seq_length = 100  # Experiment between 50 and 500\n",
        "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters: \n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256 \n",
        "rnn_units = 1024  # Experiment between 1 and 2048\n",
        "\n",
        "# Checkpoint location: \n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZolpDULcgfg"
      },
      "source": [
        "### Define optimizer and training operation ###\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = model(x)\n",
        "\n",
        "    loss = compute_loss(y, y_hat)\n",
        "  \n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'): \n",
        "  tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "for iter in tqdm(range(num_training_iterations)):\n",
        "\n",
        "  # Grab a batch and propagate it through the network\n",
        "  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
        "  loss = train_step(x_batch, y_batch)\n",
        "\n",
        "  # Update the progress bar\n",
        "  history.append(loss.numpy().mean())\n",
        "  plotter.plot(history)\n",
        "\n",
        "  # Update the model with the changed weights!\n",
        "  if iter % 100 == 0:     \n",
        "    model.save_weights(checkpoint_prefix)\n",
        "    \n",
        "# Save the trained model and the weights\n",
        "model.save_weights(checkpoint_prefix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnERJzPZeiTj"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # TODO\n",
        "# model = build_model('''TODO''', '''TODO''', '''TODO''', batch_size=1)\n",
        "\n",
        "# Restore the model weights for the last checkpoint after training\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWB3xsBsrslB"
      },
      "source": [
        "### Prediction of a generated song ###\n",
        "\n",
        "def generate_text(model, start_string, generation_length=1000):\n",
        "  # Evaluation step (generating ABC text using the learned RNN model)\n",
        "\n",
        "  '''TODO: convert the start string to numbers (vectorize)'''\n",
        "  input_eval = [char2idx[s] for s in start_string] # TODO\n",
        "  # input_eval = ['''TODO''']\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  tqdm._instances.clear()\n",
        "\n",
        "  for i in tqdm(range(generation_length)):\n",
        "      '''TODO: evaluate the inputs and generate the next character predictions'''\n",
        "      predictions = model(input_eval)\n",
        "      # predictions = model('''TODO''')\n",
        "      \n",
        "      # Remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      \n",
        "      '''TODO: use a multinomial distribution to sample'''\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      # predicted_id = tf.random.categorical('''TODO''', num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # Pass the prediction along with the previous hidden state\n",
        "      #   as the next inputs to the model\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      '''TODO: add the predicted character to the generated text!'''\n",
        "      # Hint: consider what format the prediction is in vs. the output\n",
        "      text_generated.append(idx2char[predicted_id]) # TODO \n",
        "      # text_generated.append('''TODO''')\n",
        "    \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_GFkSoKsfmx"
      },
      "source": [
        "'''TODO: Use the model and the function defined above to generate ABC format text of length 1000!\n",
        "    As you may notice, ABC files start with \"X\" - this may be a good start string.'''\n",
        "generated_text = generate_text(model, start_string=\"X\", generation_length=1000) # TODO\n",
        "# generated_text = generate_text('''TODO''', start_string=\"X\", generation_length=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2pUf-xhtaTD"
      },
      "source": [
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkzX2fRxsgvM"
      },
      "source": [
        "### Play back generated songs ###\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "\n",
        "for i, song in enumerate(generated_songs): \n",
        "  # Synthesize the waveform from a song\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # If its a valid song (correct syntax), lets play it! \n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fhZ6qlQIgK1"
      },
      "source": [
        "**Export to GitHub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzV9Fd-htMMs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-csaiCTJxrg"
      },
      "source": [
        "!git remote add origin https://$uname:$password@github.com/$uname/6S191.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5TMN5dxLRLo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}